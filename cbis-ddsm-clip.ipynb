{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1873742,"sourceType":"datasetVersion","datasetId":1115384}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport timm\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # <-- Add this line\n\n\n# Step 1: Define your base directories\nbase_dir = r'/kaggle/input/cbis-ddsm-breast-cancer-image-dataset'\njpg_dir = os.path.join(base_dir, 'jpeg')\ncsv_dir = os.path.join(base_dir, 'csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T18:29:41.416007Z","iopub.execute_input":"2024-10-25T18:29:41.416359Z","iopub.status.idle":"2024-10-25T18:29:48.554195Z","shell.execute_reply.started":"2024-10-25T18:29:41.416318Z","shell.execute_reply":"2024-10-25T18:29:48.553349Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Step 2: Load your CSV data\ndf_mass_train = pd.read_csv(os.path.join(csv_dir, 'mass_case_description_train_set.csv'))\n\ndf_mass_test = pd.read_csv(os.path.join(csv_dir, 'mass_case_description_test_set.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:29:48.555874Z","iopub.execute_input":"2024-10-25T18:29:48.556343Z","iopub.status.idle":"2024-10-25T18:29:48.938719Z","shell.execute_reply.started":"2024-10-25T18:29:48.556310Z","shell.execute_reply":"2024-10-25T18:29:48.937889Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Step 3: Correct the file paths in the DataFrame to point to cropped images\ndef correct_file_path(row):\n    directory = os.path.basename(os.path.dirname(row['cropped image file path']))\n    full_dir_path = os.path.join(jpg_dir, directory)\n    all_files_in_dir = os.listdir(full_dir_path)\n    \n    if len(all_files_in_dir) > 0:\n        correct_filename = all_files_in_dir[0]\n        correct_path = os.path.join(full_dir_path, correct_filename)\n    else:\n        correct_path = None\n\n    return correct_path\n\n# Apply the corrected path to your DataFrame using the cropped images\ndf_mass_train['image file path'] = df_mass_train.apply(correct_file_path, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:29:48.939886Z","iopub.execute_input":"2024-10-25T18:29:48.940222Z","iopub.status.idle":"2024-10-25T18:30:03.780763Z","shell.execute_reply.started":"2024-10-25T18:29:48.940189Z","shell.execute_reply":"2024-10-25T18:30:03.779971Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Step 4: Filter out missing files and ensure both CC and MLO views are present\ndf_mass_filtered = df_mass_train[df_mass_train['image file path'].notnull() & df_mass_train['image file path'].apply(os.path.exists)]\n\n# Ensure both CC and MLO views are present for each patient\ndf_mass_filtered = df_mass_filtered.groupby('patient_id').filter(lambda x: len(x['image view'].unique()) == 2)\n\nprint(f\"Remaining files after correction: {len(df_mass_filtered)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:03.782962Z","iopub.execute_input":"2024-10-25T18:30:03.783297Z","iopub.status.idle":"2024-10-25T18:30:04.455098Z","shell.execute_reply.started":"2024-10-25T18:30:03.783256Z","shell.execute_reply":"2024-10-25T18:30:04.454044Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Remaining files after correction: 1111\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 5: Define transformations and create DataLoaders\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:04.456391Z","iopub.execute_input":"2024-10-25T18:30:04.456702Z","iopub.status.idle":"2024-10-25T18:30:04.461937Z","shell.execute_reply.started":"2024-10-25T18:30:04.456670Z","shell.execute_reply":"2024-10-25T18:30:04.460989Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_mass_filtered.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:04.463093Z","iopub.execute_input":"2024-10-25T18:30:04.463393Z","iopub.status.idle":"2024-10-25T18:30:04.490745Z","shell.execute_reply.started":"2024-10-25T18:30:04.463362Z","shell.execute_reply":"2024-10-25T18:30:04.489890Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"  patient_id  breast_density left or right breast image view  abnormality id  \\\n0    P_00001               3                 LEFT         CC               1   \n1    P_00001               3                 LEFT        MLO               1   \n2    P_00004               3                 LEFT         CC               1   \n3    P_00004               3                 LEFT        MLO               1   \n4    P_00004               3                RIGHT        MLO               1   \n\n  abnormality type                          mass shape   mass margins  \\\n0             mass  IRREGULAR-ARCHITECTURAL_DISTORTION     SPICULATED   \n1             mass  IRREGULAR-ARCHITECTURAL_DISTORTION     SPICULATED   \n2             mass            ARCHITECTURAL_DISTORTION    ILL_DEFINED   \n3             mass            ARCHITECTURAL_DISTORTION    ILL_DEFINED   \n4             mass                                OVAL  CIRCUMSCRIBED   \n\n   assessment  pathology  subtlety  \\\n0           4  MALIGNANT         4   \n1           4  MALIGNANT         4   \n2           4     BENIGN         3   \n3           4     BENIGN         3   \n4           4     BENIGN         5   \n\n                                     image file path  \\\n0  /kaggle/input/cbis-ddsm-breast-cancer-image-da...   \n1  /kaggle/input/cbis-ddsm-breast-cancer-image-da...   \n2  /kaggle/input/cbis-ddsm-breast-cancer-image-da...   \n3  /kaggle/input/cbis-ddsm-breast-cancer-image-da...   \n4  /kaggle/input/cbis-ddsm-breast-cancer-image-da...   \n\n                             cropped image file path  \\\n0  Mass-Training_P_00001_LEFT_CC_1/1.3.6.1.4.1.95...   \n1  Mass-Training_P_00001_LEFT_MLO_1/1.3.6.1.4.1.9...   \n2  Mass-Training_P_00004_LEFT_CC_1/1.3.6.1.4.1.95...   \n3  Mass-Training_P_00004_LEFT_MLO_1/1.3.6.1.4.1.9...   \n4  Mass-Training_P_00004_RIGHT_MLO_1/1.3.6.1.4.1....   \n\n                                  ROI mask file path  \n0  Mass-Training_P_00001_LEFT_CC_1/1.3.6.1.4.1.95...  \n1  Mass-Training_P_00001_LEFT_MLO_1/1.3.6.1.4.1.9...  \n2  Mass-Training_P_00004_LEFT_CC_1/1.3.6.1.4.1.95...  \n3  Mass-Training_P_00004_LEFT_MLO_1/1.3.6.1.4.1.9...  \n4  Mass-Training_P_00004_RIGHT_MLO_1/1.3.6.1.4.1....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>breast_density</th>\n      <th>left or right breast</th>\n      <th>image view</th>\n      <th>abnormality id</th>\n      <th>abnormality type</th>\n      <th>mass shape</th>\n      <th>mass margins</th>\n      <th>assessment</th>\n      <th>pathology</th>\n      <th>subtlety</th>\n      <th>image file path</th>\n      <th>cropped image file path</th>\n      <th>ROI mask file path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P_00001</td>\n      <td>3</td>\n      <td>LEFT</td>\n      <td>CC</td>\n      <td>1</td>\n      <td>mass</td>\n      <td>IRREGULAR-ARCHITECTURAL_DISTORTION</td>\n      <td>SPICULATED</td>\n      <td>4</td>\n      <td>MALIGNANT</td>\n      <td>4</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n      <td>Mass-Training_P_00001_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n      <td>Mass-Training_P_00001_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P_00001</td>\n      <td>3</td>\n      <td>LEFT</td>\n      <td>MLO</td>\n      <td>1</td>\n      <td>mass</td>\n      <td>IRREGULAR-ARCHITECTURAL_DISTORTION</td>\n      <td>SPICULATED</td>\n      <td>4</td>\n      <td>MALIGNANT</td>\n      <td>4</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n      <td>Mass-Training_P_00001_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n      <td>Mass-Training_P_00001_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P_00004</td>\n      <td>3</td>\n      <td>LEFT</td>\n      <td>CC</td>\n      <td>1</td>\n      <td>mass</td>\n      <td>ARCHITECTURAL_DISTORTION</td>\n      <td>ILL_DEFINED</td>\n      <td>4</td>\n      <td>BENIGN</td>\n      <td>3</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n      <td>Mass-Training_P_00004_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n      <td>Mass-Training_P_00004_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P_00004</td>\n      <td>3</td>\n      <td>LEFT</td>\n      <td>MLO</td>\n      <td>1</td>\n      <td>mass</td>\n      <td>ARCHITECTURAL_DISTORTION</td>\n      <td>ILL_DEFINED</td>\n      <td>4</td>\n      <td>BENIGN</td>\n      <td>3</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n      <td>Mass-Training_P_00004_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n      <td>Mass-Training_P_00004_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P_00004</td>\n      <td>3</td>\n      <td>RIGHT</td>\n      <td>MLO</td>\n      <td>1</td>\n      <td>mass</td>\n      <td>OVAL</td>\n      <td>CIRCUMSCRIBED</td>\n      <td>4</td>\n      <td>BENIGN</td>\n      <td>5</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n      <td>Mass-Training_P_00004_RIGHT_MLO_1/1.3.6.1.4.1....</td>\n      <td>Mass-Training_P_00004_RIGHT_MLO_1/1.3.6.1.4.1....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_mass_filtered = df_mass_filtered[['pathology', 'image file path']]\ndf_mass_filtered.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:04.491713Z","iopub.execute_input":"2024-10-25T18:30:04.492009Z","iopub.status.idle":"2024-10-25T18:30:04.502403Z","shell.execute_reply.started":"2024-10-25T18:30:04.491978Z","shell.execute_reply":"2024-10-25T18:30:04.501472Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   pathology                                    image file path\n0  MALIGNANT  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n1  MALIGNANT  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n2     BENIGN  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n3     BENIGN  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n4     BENIGN  /kaggle/input/cbis-ddsm-breast-cancer-image-da...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pathology</th>\n      <th>image file path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MALIGNANT</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MALIGNANT</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BENIGN</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>BENIGN</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BENIGN</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def map_pathology(value):\n    if value == 'MALIGNANT':\n        return 1\n    else:  # 'BENIGN' or 'NORMAL'\n        return 0\n\ndf_mass_filtered['pathology'] = df_mass_filtered['pathology'].apply(map_pathology)\n\ndf_mass_filtered.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:04.503722Z","iopub.execute_input":"2024-10-25T18:30:04.504391Z","iopub.status.idle":"2024-10-25T18:30:04.516801Z","shell.execute_reply.started":"2024-10-25T18:30:04.504348Z","shell.execute_reply":"2024-10-25T18:30:04.515976Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   pathology                                    image file path\n0          1  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n1          1  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n2          0  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n3          0  /kaggle/input/cbis-ddsm-breast-cancer-image-da...\n4          0  /kaggle/input/cbis-ddsm-breast-cancer-image-da...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pathology</th>\n      <th>image file path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>/kaggle/input/cbis-ddsm-breast-cancer-image-da...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_mass_filtered.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:04.517974Z","iopub.execute_input":"2024-10-25T18:30:04.518297Z","iopub.status.idle":"2024-10-25T18:30:04.529033Z","shell.execute_reply.started":"2024-10-25T18:30:04.518266Z","shell.execute_reply":"2024-10-25T18:30:04.528086Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"pathology          0\nimage file path    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"!pip install open_clip_torch","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:04.533145Z","iopub.execute_input":"2024-10-25T18:30:04.533505Z","iopub.status.idle":"2024-10-25T18:30:17.753052Z","shell.execute_reply.started":"2024-10-25T18:30:04.533472Z","shell.execute_reply":"2024-10-25T18:30:17.751991Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting open_clip_torch\n  Downloading open_clip_torch-2.28.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.19.0)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2024.5.15)\nCollecting ftfy (from open_clip_torch)\n  Downloading ftfy-6.3.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.4)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.25.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.4.5)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (1.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2024.6.1)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (10.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\nDownloading open_clip_torch-2.28.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\nSuccessfully installed ftfy-6.3.0 open_clip_torch-2.28.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport open_clip\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:17.754441Z","iopub.execute_input":"2024-10-25T18:30:17.754757Z","iopub.status.idle":"2024-10-25T18:30:19.005279Z","shell.execute_reply.started":"2024-10-25T18:30:17.754722Z","shell.execute_reply":"2024-10-25T18:30:19.004408Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set device for model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:19.006414Z","iopub.execute_input":"2024-10-25T18:30:19.006916Z","iopub.status.idle":"2024-10-25T18:30:19.045507Z","shell.execute_reply.started":"2024-10-25T18:30:19.006881Z","shell.execute_reply":"2024-10-25T18:30:19.044416Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load CLIP model and preprocessing using open_clip\nmodel, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion400m_e32\")\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:19.046998Z","iopub.execute_input":"2024-10-25T18:30:19.047298Z","iopub.status.idle":"2024-10-25T18:30:30.704592Z","shell.execute_reply.started":"2024-10-25T18:30:19.047261Z","shell.execute_reply":"2024-10-25T18:30:30.702820Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 605M/605M [00:06<00:00, 93.2MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define dataset class\nclass CLIPDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.706146Z","iopub.execute_input":"2024-10-25T18:30:30.706602Z","iopub.status.idle":"2024-10-25T18:30:30.715563Z","shell.execute_reply.started":"2024-10-25T18:30:30.706565Z","shell.execute_reply":"2024-10-25T18:30:30.714144Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess data\nimage_paths = df_mass_filtered['image file path'].tolist()  # List of image file paths\nlabels = df_mass_filtered['pathology'].tolist()       # List of labels (0 for Benign/Normal, 1 for Malignant)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.717461Z","iopub.execute_input":"2024-10-25T18:30:30.717874Z","iopub.status.idle":"2024-10-25T18:30:30.729313Z","shell.execute_reply.started":"2024-10-25T18:30:30.717827Z","shell.execute_reply":"2024-10-25T18:30:30.728015Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Apply CLIP's preprocessing\ntransform = transforms.Compose([\n    preprocess,\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.730721Z","iopub.execute_input":"2024-10-25T18:30:30.731355Z","iopub.status.idle":"2024-10-25T18:30:30.741277Z","shell.execute_reply.started":"2024-10-25T18:30:30.731301Z","shell.execute_reply":"2024-10-25T18:30:30.740205Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create dataset and dataloader\ndataset = CLIPDataset(image_paths, labels, transform=transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.743819Z","iopub.execute_input":"2024-10-25T18:30:30.744254Z","iopub.status.idle":"2024-10-25T18:30:30.753766Z","shell.execute_reply.started":"2024-10-25T18:30:30.744210Z","shell.execute_reply":"2024-10-25T18:30:30.752415Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Define CLIP text tokens for classification labels\nclass_names = [\"a photo of a benign tumor\", \"a photo of a malignant tumor\"]\ntext_inputs = open_clip.tokenize(class_names).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.755229Z","iopub.execute_input":"2024-10-25T18:30:30.756077Z","iopub.status.idle":"2024-10-25T18:30:30.777798Z","shell.execute_reply.started":"2024-10-25T18:30:30.756042Z","shell.execute_reply":"2024-10-25T18:30:30.776867Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Training setup (parameters can be adjusted)\nnum_epochs = 5\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.779702Z","iopub.execute_input":"2024-10-25T18:30:30.780523Z","iopub.status.idle":"2024-10-25T18:30:30.786653Z","shell.execute_reply.started":"2024-10-25T18:30:30.780479Z","shell.execute_reply":"2024-10-25T18:30:30.785784Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    for images, labels in dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass: calculate CLIP features for images and text\n        image_features = model.encode_image(images)\n        text_features = model.encode_text(text_inputs)\n        \n        # Calculate logits and loss\n        logits_per_image = image_features @ text_features.T  # Matrix multiplication for similarity\n        loss = loss_fn(logits_per_image, labels)\n        epoch_loss += loss.item()\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n\n# Evaluation\nmodel.eval()\nall_preds = []\nall_labels = []\nwith torch.no_grad():\n    for images, labels in dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        image_features = model.encode_image(images)\n        logits_per_image = image_features @ text_features.T  # Matrix multiplication for similarity\n        probs = logits_per_image.softmax(dim=-1)\n        preds = probs.argmax(dim=-1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Performance metrics\naccuracy = accuracy_score(all_labels, all_preds)\nreport = classification_report(all_labels, all_preds, target_names=class_names)\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"Classification Report:\\n\", report)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:30:30.788018Z","iopub.execute_input":"2024-10-25T18:30:30.788384Z","iopub.status.idle":"2024-10-25T18:40:34.243827Z","shell.execute_reply.started":"2024-10-25T18:30:30.788339Z","shell.execute_reply":"2024-10-25T18:40:34.242757Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/5, Loss: 0.9334361689431326\nEpoch 2/5, Loss: 0.6779544132096427\nEpoch 3/5, Loss: 0.6122253554207938\nEpoch 4/5, Loss: 0.5542237545762744\nEpoch 5/5, Loss: 0.36146032001291\nAccuracy: 0.9298\nClassification Report:\n                               precision    recall  f1-score   support\n\n   a photo of a benign tumor       0.96      0.90      0.93       570\na photo of a malignant tumor       0.90      0.96      0.93       541\n\n                    accuracy                           0.93      1111\n                   macro avg       0.93      0.93      0.93      1111\n                weighted avg       0.93      0.93      0.93      1111\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}